ğŸš€ High-Performance Analytics Backend System
A scalable, event-driven analytics platform designed to handle high-volume event ingestion with real-time processing and comprehensive reporting capabilities.
ğŸ“‹ Table of Contents

Architecture Overview
System Design
Database Schema
Prerequisites
Quick Start
Manual Setup
API Documentation
Testing
Performance Considerations
Troubleshooting


ğŸ—ï¸ Architecture Overview
The system consists of three microservices working together:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client Apps   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SERVICE 1: Ingestion API                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  POST /event - Ultra-fast event acceptance       â”‚  â”‚
â”‚  â”‚  â€¢ Minimal validation (< 1ms)                    â”‚  â”‚
â”‚  â”‚  â€¢ Immediate 202 Accepted response               â”‚  â”‚
â”‚  â”‚  â€¢ Push to Redis queue (async)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Redis  â”‚ â† Queue (FIFO)
    â”‚ Queue  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SERVICE 2: Processor Worker                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Pull events from queue (batch processing)     â”‚  â”‚
â”‚  â”‚  â€¢ Batch insert into PostgreSQL                  â”‚  â”‚
â”‚  â”‚  â€¢ Update aggregation tables                     â”‚  â”‚
â”‚  â”‚  â€¢ Process 100 events/batch                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PostgreSQL   â”‚
  â”‚ Database     â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SERVICE 3: Reporting API                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  GET /stats - Aggregate statistics               â”‚  â”‚
â”‚  â”‚  â€¢ Fast queries using pre-aggregated tables      â”‚  â”‚
â”‚  â”‚  â€¢ Multiple report endpoints                     â”‚  â”‚
â”‚  â”‚  â€¢ Real-time and historical data                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ¯ Why This Architecture?
1. Ingestion Speed Priority

The ingestion API responds in < 5ms by immediately accepting events and queuing them
No database I/O blocking the response
Client doesn't wait for processing

2. Asynchronous Processing with Redis

Redis acts as a message queue between ingestion and processing
Decouples ingestion from database writes
Handles traffic spikes gracefully
In-memory speed for queue operations

3. Batch Processing

Processor pulls events in batches of 100
Single transaction for entire batch
Reduces database connection overhead
Better throughput than single inserts

4. Pre-aggregated Reporting

Separate tables for daily stats and path stats
Queries are fast (< 50ms) even with millions of events
No need to scan entire events table for reports


ğŸ—„ï¸ Database Schema
Tables
1. events (Raw Events)
Stores every single event received.
sqlCREATE TABLE events (
    id BIGSERIAL PRIMARY KEY,
    site_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    path VARCHAR(500),
    user_id VARCHAR(255),
    timestamp TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes for fast queries
CREATE INDEX idx_events_site_id ON events(site_id);
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_site_timestamp ON events(site_id, timestamp);
2. daily_stats (Aggregated Daily Data)
Pre-calculated daily statistics for each site.
sqlCREATE TABLE daily_stats (
    id BIGSERIAL PRIMARY KEY,
    site_id VARCHAR(255) NOT NULL,
    date DATE NOT NULL,
    total_views INTEGER DEFAULT 0,
    unique_users INTEGER DEFAULT 0,
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(site_id, date)
);
3. path_stats (Path-level Analytics)
View counts per path, per day, per site.
sqlCREATE TABLE path_stats (
    id BIGSERIAL PRIMARY KEY,
    site_id VARCHAR(255) NOT NULL,
    date DATE NOT NULL,
    path VARCHAR(500) NOT NULL,
    views INTEGER DEFAULT 0,
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(site_id, date, path)
);
Database Optimization

UNIQUE constraints enable UPSERT operations (INSERT ... ON CONFLICT)
Indexes on frequently queried columns (site_id, date, timestamp)
BIGSERIAL for high-volume ID generation
Partial indexes can be added for specific query patterns


ğŸ“¦ Prerequisites
Before you begin, ensure you have:

Docker (v20.10 or higher) and Docker Compose (v2.0 or higher)
Node.js (v18 or higher) - for local development only
Git - to clone the repository
curl or Postman - for API testing

Check Prerequisites
bash# Check Docker
docker --version
docker-compose --version

# Check Node.js (optional, for local dev)
node --version
npm --version

ğŸš€ Quick Start (Docker)
1. Clone the Repository
bashgit clone <repository-url>
cd analytics-backend
2. Start All Services
bashdocker-compose up --build
This command will:

Build all three services
Start PostgreSQL database
Start Redis queue
Initialize database schema
Start Ingestion API (port 3001)
Start Processor Worker
Start Reporting API (port 3002)

3. Verify Services are Running
bash# Check Ingestion API
curl http://localhost:3001/health

# Check Reporting API
curl http://localhost:3002/health
Expected Response:
json{
  "status": "healthy",
  "service": "ingestion",
  "queue": {
    "connected": true,
    "length": 0
  }
}

ğŸ› ï¸ Manual Setup (Without Docker)
Step 1: Install Dependencies
PostgreSQL
bash# macOS
brew install postgresql@15
brew services start postgresql@15

# Ubuntu/Debian
sudo apt-get install postgresql-15
sudo systemctl start postgresql

# Windows
# Download from https://www.postgresql.org/download/windows/
Redis
bash# macOS
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt-get install redis-server
sudo systemctl start redis

# Windows
# Download from https://github.com/microsoftarchive/redis/releases
Step 2: Setup Database
bash# Create database and user
psql -U postgres

CREATE DATABASE analytics;
CREATE USER analytics_user WITH PASSWORD 'analytics_pass';
GRANT ALL PRIVILEGES ON DATABASE analytics TO analytics_user;
\q

# Initialize schema
psql -U analytics_user -d analytics -f database/init.sql
Step 3: Install Service Dependencies
bash# Ingestion Service
cd services/ingestion
npm install
cp .env.example .env
# Edit .env with your configuration

# Processor Service
cd ../processor
npm install
cp .env.example .env
# Edit .env with your configuration

# Reporting Service
cd ../reporting
npm install
cp .env.example .env
# Edit .env with your configuration
Step 4: Start Services
Open three terminal windows:
bash# Terminal 1 - Ingestion API
cd services/ingestion
npm start

# Terminal 2 - Processor Worker
cd services/processor
npm start

# Terminal 3 - Reporting API
cd services/reporting
npm start

ğŸ“š API Documentation
Service 1: Ingestion API (Port 3001)
POST /event
Submit a single analytics event.
Request:
bashcurl -X POST http://localhost:3001/event \
  -H "Content-Type: application/json" \
  -d '{
    "site_id": "site-abc-123",
    "event_type": "page_view",
    "path": "/pricing",
    "user_id": "user-xyz-789",
    "timestamp": "2025-11-12T19:30:01Z"
  }'
Response (202 Accepted):
json{
  "success": true,
  "message": "Event accepted for processing"
}
Performance: Responds in < 5ms
POST /events/batch
Submit multiple events at once (up to 1000).
Request:
bashcurl -X POST http://localhost:3001/events/batch \
  -H "Content-Type: application/json" \
  -d '{
    "events": [
      {
        "site_id": "site-abc-123",
        "event_type": "page_view",
        "path": "/home",
        "user_id": "user-1",
        "timestamp": "2025-11-12T19:30:01Z"
      },
      {
        "site_id": "site-abc-123",
        "event_type": "page_view",
        "path": "/about",
        "user_id": "user-2",
        "timestamp": "2025-11-12T19:30:05Z"
      }
    ]
  }'
GET /health
Health check endpoint.
bashcurl http://localhost:3001/health

Service 3: Reporting API (Port 3002)
GET /stats
Get statistics for a site.
All-time stats:
bashcurl "http://localhost:3002/stats?site_id=site-abc-123"
Specific date:
bashcurl "http://localhost:3002/stats?site_id=site-abc-123&date=2025-11-12"
Response:
json{
  "site_id": "site-abc-123",
  "date": "2025-11-12",
  "total_views": 1450,
  "unique_users": 212,
  "top_paths": [
    {
      "path": "/pricing",
      "views": 700
    },
    {
      "path": "/blog/post-1",
      "views": 500
    },
    {
      "path": "/",
      "views": 250
    }
  ]
}
GET /stats/overview
Comprehensive site overview.
bashcurl "http://localhost:3002/stats/overview?site_id=site-abc-123"
Response:
json{
  "site_id": "site-abc-123",
  "today": {
    "total_views": 450,
    "unique_users": 89
  },
  "yesterday": {
    "total_views": 523,
    "unique_users": 95
  },
  "all_time": {
    "total_views": 45230,
    "unique_users": 3421
  },
  "last_7_days": [...]
}
GET /stats/trending
Get trending paths (highest growth).
bashcurl "http://localhost:3002/stats/trending?site_id=site-abc-123&days=7"
GET /stats/realtime
Get real-time statistics (last N minutes).
bashcurl "http://localhost:3002/stats/realtime?site_id=site-abc-123&minutes=30"
GET /stats/range
Get statistics for a date range.
bashcurl "http://localhost:3002/stats/range?site_id=site-abc-123&start_date=2025-11-01&end_date=2025-11-12"

ğŸ§ª Testing
Load Testing the Ingestion API
Create a test script load-test.sh:
bash#!/bin/bash

# Generate 1000 events
for i in {1..1000}; do
  curl -X POST http://localhost:3001/event \
    -H "Content-Type: application/json" \
    -d "{
      \"site_id\": \"site-test-123\",
      \"event_type\": \"page_view\",
      \"path\": \"/test-page-$i\",
      \"user_id\": \"user-$((RANDOM % 100))\",
      \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
    }" &
done

wait
echo "âœ“ Sent 1000 events"
Run the test:
bashchmod +x load-test.sh
./load-test.sh
Verify Data Processing
bash# Wait a few seconds for processing
sleep 10

# Check stats
curl "http://localhost:3002/stats?site_id=site-test-123"
Database Verification
bash# Connect to database
docker exec -it analytics_postgres psql -U analytics_user -d analytics

# Check event count
SELECT COUNT(*) FROM events WHERE site_id = 'site-test-123';

# Check daily stats
SELECT * FROM daily_stats WHERE site_id = 'site-test-123';

# Check top paths
SELECT * FROM path_stats WHERE site_id = 'site-test-123' ORDER BY views DESC LIMIT 10;

âš¡ Performance Considerations
Ingestion API

Response Time: < 5ms average
Throughput: 10,000+ requests/second per instance
Scalability: Horizontally scalable (add more instances behind load balancer)

Queue (Redis)

Latency: < 1ms for LPUSH/BRPOP operations
Throughput: Millions of messages per second
Persistence: AOF (Append-Only File) enabled for durability

Processor Worker

Batch Size: 100 events per batch
Processing Time: ~50ms per batch (database insert)
Throughput: ~2000 events/second per worker
Scalability: Run multiple worker instances for higher throughput

Database

Insert Performance: Batch inserts (100 events) in single transaction
Query Performance: < 50ms for aggregated stats queries
Storage: Efficiently stores millions of events
Optimization: Indexes on frequently queried columns

Scaling Strategies
Horizontal Scaling:
yaml# docker-compose.yml
processor:
  deploy:
    replicas: 5  # Run 5 processor instances
Database Partitioning:
sql-- Partition events table by month
CREATE TABLE events_2025_11 PARTITION OF events
FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');
Caching:

Add Redis cache for frequently accessed stats
Cache TTL: 1-5 minutes


ğŸ› Troubleshooting
Issue: Services won't start
Solution:
bash# Check if ports are in use
lsof -i :3001
lsof -i :3002
lsof -i :5432
lsof -i :6379

# Stop conflicting services or change ports in .env
Issue: Events not appearing in database
Check processor logs:
bashdocker logs analytics_processor
Check queue length:
bashdocker exec -it analytics_redis redis-cli
LLEN analytics:events
Verify database connection:
bashdocker exec -it analytics_postgres psql -U analytics_user -d analytics -c "SELECT COUNT(*) FROM events;"
Issue: Slow ingestion
Check Redis connection:
bashcurl http://localhost:3001/health
Monitor queue:
bash# If queue length keeps growing, add more processor workers
docker-compose up --scale processor=3
Issue: Stats not updating
Recalculate unique users:
sqlSELECT recalculate_unique_users('site-abc-123', '2025-11-12');
Check aggregation tables:
sqlSELECT * FROM daily_stats WHERE site_id = 'site-abc-123' ORDER BY date DESC;
SELECT * FROM path_stats WHERE site_id = 'site-abc-123' ORDER BY views DESC LIMIT 10;

ğŸ“Š Monitoring
Key Metrics to Track
Ingestion API:

Request rate (requests/second)
Response time (p50, p95, p99)
Error rate
Queue push success rate

Processor:

Events processed per second
Batch processing time
Queue depth
Database insert latency
Error count

Reporting API:

Query response time
Cache hit rate (if caching enabled)
Error rate

Monitoring Tools
Recommended Stack:

Prometheus (metrics collection)
Grafana (visualization)
ELK Stack (logs)
New Relic / Datadog (APM)


ğŸš€ Production Deployment
Environment Variables
Create production .env files for each service:
bash# Ingestion Service
NODE_ENV=production
PORT=3001
REDIS_HOST=redis.production.com
REDIS_PORT=6379

# Processor Service
NODE_ENV=production
REDIS_HOST=redis.production.com
DB_HOST=postgres.production.com
DB_PASSWORD=<secure-password>

# Reporting Service
NODE_ENV=production
PORT=3002
DB_HOST=postgres.production.com
DB_PASSWORD=<secure-password>
Security Checklist

 Use environment-specific credentials
 Enable SSL/TLS for database connections
 Add rate limiting to ingestion API
 Implement API authentication (JWT, API keys)
 Enable HTTPS (reverse proxy with nginx)
 Set up firewall rules
 Regular security updates
 Database backup strategy


ğŸ“„ License
MIT License - see LICENSE file for details

ğŸ‘¥ Contributing
Contributions are welcome! Please read CONTRIBUTING.md for guidelines.

ğŸ“ Support
For issues and questions:

Open an issue on GitHub
Email: sahilburele6789@gmail.com
Documentation: https://docs.example.com


Built with â¤ï¸ using Node.js, Redis, and PostgreSQL